{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13a78c0-1429-439c-927d-d7ccf52790df",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "Em suma, trata-se de uma sub √°rea de Processamento de Linguagem Natural (NLP) que lida com o problema de responder perguntas escritas em linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4711ac-330e-44ef-ba7e-9ad05b4895ae",
   "metadata": {},
   "source": [
    "## ü§ó Reposit√≥rio de Modelos (_Model Hub_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19728771-4e1c-4404-8b6c-fed91dfbee70",
   "metadata": {},
   "source": [
    "https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c1c10-e141-4a8f-b9d7-4d892f1e5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09db85-a3ab-4ae6-8a3f-e96f760b6316",
   "metadata": {},
   "source": [
    "## 1. Selecione um modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05457d4-0b31-446d-abd8-cfa798c162ee",
   "metadata": {},
   "source": [
    "O primeiro passo √© selecionar um modelo do hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5f8eb-43ba-4831-83fb-d2cab6e508b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2d85d-9750-4c6d-990e-fe62a22d824b",
   "metadata": {},
   "source": [
    "## 2. Crie um pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62664c76-760b-440a-8122-9a33838899d6",
   "metadata": {},
   "source": [
    "Em seguida, criamos um pipeline. Um pipeline encapsula a cria√ß√£o do modelo espec√≠fico para uma determinada tarefa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc9fec-f323-45b5-ac12-0f6e7581e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipe = pipeline(\"question-answering\", model = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc03a3-608a-4459-af30-8b142a32b17e",
   "metadata": {},
   "source": [
    "## 3. Defina um contexto e uma pergunta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1d0c5-b7d1-4ce3-839c-0bd4bfbd2f19",
   "metadata": {},
   "source": [
    "Em _question answering_, fornecemos ao modelo um contexto e uma pergunta. A resposta fornecida ser√° constru√≠da a partir deste contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a2f30-5715-4507-b946-43512a3f4d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing\n",
    "(NLP) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google.[1][2]\n",
    "In 2019, Google announced that it had begun leveraging BERT in its search engine, and by late 2020 it was using BERT in almost every English-language query.\n",
    "A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in NLP experiments\", \n",
    "counting over 150 research publications analyzing and improving the model.[3]\n",
    "\n",
    "The original English-language BERT has two models:[1] (1) the BERTBASE: 12 encoders with 12 bidirectional self-attention heads, and (2) the \n",
    "BERTLARGE: 24 encoders with 16 bidirectional self-attention heads. Both models are pre-trained from unlabeled data extracted from the BooksCorpus[4] \n",
    "with 800M words and English Wikipedia with 2,500M words.[5]\n",
    "\"\"\"\n",
    "\n",
    "question = \"What does BERT mean?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa82cbc-f287-4cea-86c9-1ab8c867edf2",
   "metadata": {},
   "source": [
    "## 4. Fa√ßa a pergunta ao modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49170fb9-8bce-44db-9278-644115082f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = qa_pipe(question=question, context=context)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Score:\", result['score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
